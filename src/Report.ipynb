{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Instrument detection is a cornerstone in music information retrieval (MIR). In this notebook, we will explore the task of instrument detection and train varying models. We will follow the method of [Predominant Musical Instrument Classification based on Spectral Features](https://arxiv.org/pdf/1912.02606.pdf) (MIC) by Racharla et al. (2020). The dataset used in this notebook is the [OpenMIC-2018](http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf), a crowd-sourced dataset of 20,000 audio files with 10-second clips of 20 different instruments, sponsored by Spotify and New York Univiversity's MARL and Center for Data Science. The dataset is available on [paperswithcode.com](https://paperswithcode.com/dataset/openmic-2018). They also have a [github repo](https://github.com/cosmir/openmic-2018) with the dataset and [modelling baseline code](https://github.com/cosmir/openmic-2018/blob/master/examples/modeling-baseline.ipynb).\n",
    "\n",
    "The OpenMIC-2018 dataset were chosen before we understood the limitations of the dataset, and so, our final model and analysis will have some limitations. The comparions with MIC are not going to be fair, which is furthered discussed in later sections. All together, it has been a great learning experience.\n",
    "\n",
    "This project is a part of [DT2470 Music Informatics](https://www.kth.se/student/kurser/kurs/DT2470?l=en) at KTH Royal Institute of Technology. The time from start to finish was approximately three weeks. \n",
    "\n",
    "Let's load some of the necessary libraries and get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we preprocess the data, and extract features from it. Then, we train and evaluate the models. The OpenMIC-2018 dataset consists of 20 000 audio samples, but not all of them have human annotations. If annotations are missing, that specific instrument and sample can't be included in model traning and testing. It was a bit cumbersome to understand the design of the dataset, but the modelling baseline was really helpful. The dataset comes with a predefined `.npz` file. It also includeds a well-balanced train and test split of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the `DATA_ROOT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '../data/openmic-2018/'\n",
    "\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    raise ValueError('Did you forget to set `DATA_ROOT`?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `OPENMIC` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "OPENMIC = np.load(os.path.join(DATA_ROOT, 'openmic-2018.npz'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the dataset is described in detail on the [OPENMIC's Official Github](https://github.com/cosmir/openmic-2018/blob/master/examples/modeling-baseline.ipynb). It contains VGGish features, which are disregarded, as we want to replicate the analysis of [MIC](https://arxiv.org/abs/1912.02606) and extract our own features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y_true, Y_mask, sample_key = OPENMIC['X'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "The goal is to replicate the approach of [MIC](https://arxiv.org/abs/1912.02606). The features used in the article are:\n",
    "\n",
    "* **Zero Crossing Rate** (ZCR):  indicates the rate at which the signal crosses zero.\n",
    "* **Spectral Centroid** (SC): a metric that indicate the center of mass of the spectrum being located. It is *\"the ratio of the frequency weighted magnitude spectrum with unweighted magnitude spectrum\"*\n",
    "* **Spectral Bandwidth** (SB): gives the weighted average of the frequency signal by its spectrum.\n",
    "* **Spectral roll-off** (SR):  the frequency under which a certain proportion of the overall spectral energy belongs to\n",
    "* **MFCC**: the mean of the first 13 MFCC features.\n",
    "\n",
    "We intend to  extract these features for OpenMIC-2018, and store them in a Panda's Dataframe ordered by sample key. The sample keys are available in the `sample_key` list produced in the code block above. The dataframe is then exported as a `.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From PMICSF, page 3:\n",
    "\n",
    ">  The audio spectrum is analyzed by extracting MFCCs based on the default inputs of hopSize (hop length between frames) and frame size. The default parameters for sampling rate is 44.1 kHz, **hopSize of 512** and **frame size of 1024** in Essentia\n",
    "\n",
    "They don't mention what was used for Librosa, but we decided to pick these parameter values as they are quite common and make sense in this context. For convinience, and according to [Presets](https://librosa.org/blog/2019/07/17/resample-on-load/), we set the default parameters of Librosa to match those values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the default values to match MIC\n",
    "from presets import Preset\n",
    "import librosa as _librosa\n",
    "librosa = Preset(_librosa)\n",
    "librosa['n_fft'] = 2048\n",
    "librosa['win_length'] = 1024\n",
    "librosa['hop_length'] = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find all audio files in the dataset, Librosa's util function [find_files](https://librosa.org/doc/main/generated/librosa.util.find_files.html) was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>sample_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>000046_3840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>000135_483840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>000139_119040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>000141_153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>000144_30720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>155294_184320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>155295_76800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>155307_211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>155310_372480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>/workspaces/instrument-detection/data/openmic-...</td>\n",
       "      <td>155311_453120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               file_path     sample_key\n",
       "0      /workspaces/instrument-detection/data/openmic-...    000046_3840\n",
       "1      /workspaces/instrument-detection/data/openmic-...  000135_483840\n",
       "2      /workspaces/instrument-detection/data/openmic-...  000139_119040\n",
       "3      /workspaces/instrument-detection/data/openmic-...  000141_153600\n",
       "4      /workspaces/instrument-detection/data/openmic-...   000144_30720\n",
       "...                                                  ...            ...\n",
       "19995  /workspaces/instrument-detection/data/openmic-...  155294_184320\n",
       "19996  /workspaces/instrument-detection/data/openmic-...   155295_76800\n",
       "19997  /workspaces/instrument-detection/data/openmic-...  155307_211200\n",
       "19998  /workspaces/instrument-detection/data/openmic-...  155310_372480\n",
       "19999  /workspaces/instrument-detection/data/openmic-...  155311_453120\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa as lr\n",
    "import pandas as pd\n",
    "\n",
    "file_paths = lr.util.find_files(DATA_ROOT + \"audio\", ext=\"ogg\")\n",
    "index = pd.DataFrame({\"file_path\": file_paths, \"sample_key\": sample_key})\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to preprocess all files and extract their features. For efficiancy, we used Spotify's [Pedalboard](https://github.com/spotify/pedalboard) for loading audio and Librosas Feature packages for feature extraction:\n",
    "\n",
    "* ZCR: [librosa.feature.zero_crossing_rate](https://librosa.org/doc/main/generated/librosa.feature.zero_crossing_rate.html)\n",
    "* SC: [librosa.feature.spectral_centroid](https://librosa.org/doc/main/generated/librosa.feature.spectral_centroid.html)\n",
    "* SB: [librosa.feature.spectral_bandwidth](https://librosa.org/doc/main/generated/librosa.feature.spectral_bandwidth.html)\n",
    "* SR: [librosa.feature.spectral_rolloff](https://librosa.org/doc/main/generated/librosa.feature.spectral_rolloff.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, the article also extracts the mean of the first 13 MFCC features with Librosa:\n",
    "\n",
    "> We extracted the first 13 MFCC features using Librosa/Essentia. For each audio clip, we obtained 259 Ã— 13 matrix features. **We took the mean of all the columns to get the condensed feature** providing us with 1 Ã— 13 feature vector, along with five other features as mentioned above. We labeled each vector with the instrument class using scikit- learnâ€™s â€˜labelencoderâ€™ function.\n",
    "\n",
    "They only use the mean to train their models. Out of curiosity, and to see if we can improve accuracy, we also save the standard deviations of all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pedalboard as pb\n",
    "import librosa as lr\n",
    "\n",
    "\n",
    "def preprocess(index):\n",
    "    \"\"\"\n",
    "    Preprocess audio and extract features according to PMICSF\n",
    "    Input: an audiofile\n",
    "    Returns: a dictionary with zcrs, scs, mfccs\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    with pb.io.AudioFile(index[0]) as f:\n",
    "        # TODO so some files have varying SR, which could be problematic\n",
    "        #assert f.samplerate == 44100, f\"Sample rate is not 44.1khz for {file}!\"\n",
    "        y = f.read(f.frames)\n",
    "        y = y.mean(axis=0)  # mono\n",
    "        # To speed up calculation, calculate one spectogram\n",
    "        S = np.abs(librosa.stft(y))**2\n",
    "        zcrs = librosa.feature.zero_crossing_rate(y=y)\n",
    "        features[\"sample_key\"] = index[1]\n",
    "        features[\"zcr_mean\"] = zcrs.mean()\n",
    "        features[\"zcr_std\"] = zcrs.std()\n",
    "        scs = librosa.feature.spectral_centroid(S=S)\n",
    "        features[\"sc_mean\"] = scs.mean()\n",
    "        features[\"sc_std\"] = scs.std()\n",
    "        sbs = librosa.feature.spectral_bandwidth(S=S)\n",
    "        features[\"sb_mean\"] = sbs.mean()\n",
    "        features[\"sb_std\"] = sbs.std()\n",
    "        srs = librosa.feature.spectral_rolloff(S=S)\n",
    "        features[\"sr_mean\"] = sbs.mean()\n",
    "        features[\"sr_std\"] = sbs.std()\n",
    "        mfccs = librosa.feature.mfcc(S=S, n_mfcc=13)\n",
    "        for i, mfcc in enumerate(mfccs):\n",
    "            features['mfcc' + str(i+1) + '_mean'] = mfcc.mean()\n",
    "            features['mfcc' + str(i+1) + 'std'] = mfcc.std()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio files were preprocessed in parallell, using Python's [multiprocess](https://docs.python.org/3/library/multiprocessing.html) package.\n",
    "\n",
    "> **NOTE:** if using the enclosed `.devcontainer`, make sure to adjust the RAM and available cores in your Docker settings. Not doing so will render the kernel to crash! We used 4gb ram and 6 cores on a MacBook Air M1, leading to a calculation time of roughly 10 minutes.\n",
    "\n",
    "Following Librosas convention of naming audiofiles `y`, we name all audio files `ys`. Think of it as \"audio files in plural\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if not os.path.exists('features.csv'):\n",
    "    with Pool() as p:\n",
    "        # index.values.tolist() returns a list with [file_path, sample_key]\n",
    "        ys = list(tqdm(p.imap(preprocess, index.values.tolist()), total=len(index)))\n",
    "        ys = pd.DataFrame(ys)\n",
    "        ys = ys.set_index(\"sample_key\")\n",
    "else:\n",
    "    ys = pd.read_csv('features.csv', index_col=\"sample_key\")\n",
    "    #ys.drop(columns=\"Unnamed: 0\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then go on to store the features in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features already calculated and stored in features.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('features.csv'):\n",
    "    pd.DataFrame(ys).to_csv('features.csv')\n",
    "else:\n",
    "    print(\"Features already calculated and stored in features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we lean heavily on the modelling baseline from the OPENMIC's Official Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '../data/openmic-2018'\n",
    "\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    raise ValueError('Did you forget to set `DATA_ROOT`?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENMIC = np.load(os.path.join(DATA_ROOT, 'openmic-2018.npz'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will overwrite 'X' later\n",
    "X, Y_true, Y_mask, sample_key = OPENMIC['X'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the previously calculated features, and print it as a sanity check. It should contain 20 000 indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_key</th>\n",
       "      <th>zcr_mean</th>\n",
       "      <th>zcr_std</th>\n",
       "      <th>sc_mean</th>\n",
       "      <th>sc_std</th>\n",
       "      <th>sb_mean</th>\n",
       "      <th>sb_std</th>\n",
       "      <th>sr_mean</th>\n",
       "      <th>sr_std</th>\n",
       "      <th>mfcc1_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc9_mean</th>\n",
       "      <th>mfcc9std</th>\n",
       "      <th>mfcc10_mean</th>\n",
       "      <th>mfcc10std</th>\n",
       "      <th>mfcc11_mean</th>\n",
       "      <th>mfcc11std</th>\n",
       "      <th>mfcc12_mean</th>\n",
       "      <th>mfcc12std</th>\n",
       "      <th>mfcc13_mean</th>\n",
       "      <th>mfcc13std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000046_3840</td>\n",
       "      <td>0.036367</td>\n",
       "      <td>0.021923</td>\n",
       "      <td>296.517136</td>\n",
       "      <td>133.109808</td>\n",
       "      <td>288.159666</td>\n",
       "      <td>267.139540</td>\n",
       "      <td>288.159666</td>\n",
       "      <td>267.139540</td>\n",
       "      <td>147.188060</td>\n",
       "      <td>...</td>\n",
       "      <td>166.07562</td>\n",
       "      <td>216.15845</td>\n",
       "      <td>158.768600</td>\n",
       "      <td>209.785170</td>\n",
       "      <td>151.586730</td>\n",
       "      <td>203.43971</td>\n",
       "      <td>144.200240</td>\n",
       "      <td>197.13676</td>\n",
       "      <td>136.577680</td>\n",
       "      <td>190.80775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000135_483840</td>\n",
       "      <td>0.052411</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>478.192757</td>\n",
       "      <td>141.199973</td>\n",
       "      <td>409.844515</td>\n",
       "      <td>135.532246</td>\n",
       "      <td>409.844515</td>\n",
       "      <td>135.532246</td>\n",
       "      <td>1282.769400</td>\n",
       "      <td>...</td>\n",
       "      <td>765.03280</td>\n",
       "      <td>495.68510</td>\n",
       "      <td>590.328800</td>\n",
       "      <td>558.758240</td>\n",
       "      <td>425.313660</td>\n",
       "      <td>616.81950</td>\n",
       "      <td>274.802150</td>\n",
       "      <td>663.55350</td>\n",
       "      <td>142.181600</td>\n",
       "      <td>694.35410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000139_119040</td>\n",
       "      <td>0.081234</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>604.505859</td>\n",
       "      <td>189.731508</td>\n",
       "      <td>719.222449</td>\n",
       "      <td>139.070343</td>\n",
       "      <td>719.222449</td>\n",
       "      <td>139.070343</td>\n",
       "      <td>292.690280</td>\n",
       "      <td>...</td>\n",
       "      <td>214.49968</td>\n",
       "      <td>316.38672</td>\n",
       "      <td>198.657150</td>\n",
       "      <td>314.673430</td>\n",
       "      <td>186.793670</td>\n",
       "      <td>313.87470</td>\n",
       "      <td>177.732700</td>\n",
       "      <td>313.87650</td>\n",
       "      <td>170.007640</td>\n",
       "      <td>313.99084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000141_153600</td>\n",
       "      <td>0.053718</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>462.825122</td>\n",
       "      <td>153.244140</td>\n",
       "      <td>391.783679</td>\n",
       "      <td>84.894649</td>\n",
       "      <td>391.783679</td>\n",
       "      <td>84.894649</td>\n",
       "      <td>188.996630</td>\n",
       "      <td>...</td>\n",
       "      <td>146.75562</td>\n",
       "      <td>140.59268</td>\n",
       "      <td>130.843980</td>\n",
       "      <td>137.768260</td>\n",
       "      <td>116.143745</td>\n",
       "      <td>135.27222</td>\n",
       "      <td>102.521700</td>\n",
       "      <td>133.01720</td>\n",
       "      <td>89.947320</td>\n",
       "      <td>130.88220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000144_30720</td>\n",
       "      <td>0.082449</td>\n",
       "      <td>0.034076</td>\n",
       "      <td>601.706717</td>\n",
       "      <td>329.074617</td>\n",
       "      <td>766.763811</td>\n",
       "      <td>375.408621</td>\n",
       "      <td>766.763811</td>\n",
       "      <td>375.408621</td>\n",
       "      <td>196.932450</td>\n",
       "      <td>...</td>\n",
       "      <td>172.74915</td>\n",
       "      <td>268.45245</td>\n",
       "      <td>166.110350</td>\n",
       "      <td>266.885220</td>\n",
       "      <td>159.133510</td>\n",
       "      <td>265.19022</td>\n",
       "      <td>151.693700</td>\n",
       "      <td>263.76523</td>\n",
       "      <td>143.998100</td>\n",
       "      <td>262.70773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>155294_184320</td>\n",
       "      <td>0.050435</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>508.286842</td>\n",
       "      <td>94.249039</td>\n",
       "      <td>266.331118</td>\n",
       "      <td>63.090107</td>\n",
       "      <td>266.331118</td>\n",
       "      <td>63.090107</td>\n",
       "      <td>147.754410</td>\n",
       "      <td>...</td>\n",
       "      <td>75.58570</td>\n",
       "      <td>66.08795</td>\n",
       "      <td>52.333115</td>\n",
       "      <td>52.021423</td>\n",
       "      <td>29.923525</td>\n",
       "      <td>44.92963</td>\n",
       "      <td>8.635664</td>\n",
       "      <td>46.96052</td>\n",
       "      <td>-11.322803</td>\n",
       "      <td>55.88632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>155295_76800</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>61.216410</td>\n",
       "      <td>42.815086</td>\n",
       "      <td>101.288169</td>\n",
       "      <td>53.390921</td>\n",
       "      <td>101.288169</td>\n",
       "      <td>53.390921</td>\n",
       "      <td>631.029240</td>\n",
       "      <td>...</td>\n",
       "      <td>866.60800</td>\n",
       "      <td>485.64580</td>\n",
       "      <td>861.181340</td>\n",
       "      <td>485.591300</td>\n",
       "      <td>855.655100</td>\n",
       "      <td>485.54526</td>\n",
       "      <td>850.119600</td>\n",
       "      <td>485.50488</td>\n",
       "      <td>844.646670</td>\n",
       "      <td>485.46692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>155307_211200</td>\n",
       "      <td>0.065545</td>\n",
       "      <td>0.022368</td>\n",
       "      <td>450.432458</td>\n",
       "      <td>192.751011</td>\n",
       "      <td>665.939788</td>\n",
       "      <td>285.751704</td>\n",
       "      <td>665.939788</td>\n",
       "      <td>285.751704</td>\n",
       "      <td>100.515335</td>\n",
       "      <td>...</td>\n",
       "      <td>87.85434</td>\n",
       "      <td>141.25677</td>\n",
       "      <td>81.044754</td>\n",
       "      <td>138.202670</td>\n",
       "      <td>74.912180</td>\n",
       "      <td>134.90385</td>\n",
       "      <td>69.371315</td>\n",
       "      <td>131.49872</td>\n",
       "      <td>64.451385</td>\n",
       "      <td>128.12172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>155310_372480</td>\n",
       "      <td>0.042145</td>\n",
       "      <td>0.015084</td>\n",
       "      <td>332.139261</td>\n",
       "      <td>146.760990</td>\n",
       "      <td>409.563611</td>\n",
       "      <td>169.775745</td>\n",
       "      <td>409.563611</td>\n",
       "      <td>169.775745</td>\n",
       "      <td>144.219150</td>\n",
       "      <td>...</td>\n",
       "      <td>139.86327</td>\n",
       "      <td>216.60248</td>\n",
       "      <td>130.948000</td>\n",
       "      <td>216.237840</td>\n",
       "      <td>122.950190</td>\n",
       "      <td>216.05978</td>\n",
       "      <td>116.407776</td>\n",
       "      <td>216.06279</td>\n",
       "      <td>111.108376</td>\n",
       "      <td>216.22318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>155311_453120</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>0.016151</td>\n",
       "      <td>300.841059</td>\n",
       "      <td>117.395470</td>\n",
       "      <td>388.035180</td>\n",
       "      <td>205.422401</td>\n",
       "      <td>388.035180</td>\n",
       "      <td>205.422401</td>\n",
       "      <td>254.896030</td>\n",
       "      <td>...</td>\n",
       "      <td>273.97130</td>\n",
       "      <td>215.41476</td>\n",
       "      <td>259.883360</td>\n",
       "      <td>210.476330</td>\n",
       "      <td>245.089900</td>\n",
       "      <td>205.54100</td>\n",
       "      <td>230.410980</td>\n",
       "      <td>201.05861</td>\n",
       "      <td>216.157270</td>\n",
       "      <td>197.14174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sample_key  zcr_mean   zcr_std     sc_mean      sc_std     sb_mean  \\\n",
       "0        000046_3840  0.036367  0.021923  296.517136  133.109808  288.159666   \n",
       "1      000135_483840  0.052411  0.013491  478.192757  141.199973  409.844515   \n",
       "2      000139_119040  0.081234  0.014925  604.505859  189.731508  719.222449   \n",
       "3      000141_153600  0.053718  0.013248  462.825122  153.244140  391.783679   \n",
       "4       000144_30720  0.082449  0.034076  601.706717  329.074617  766.763811   \n",
       "...              ...       ...       ...         ...         ...         ...   \n",
       "19995  155294_184320  0.050435  0.009352  508.286842   94.249039  266.331118   \n",
       "19996   155295_76800  0.009752  0.004809   61.216410   42.815086  101.288169   \n",
       "19997  155307_211200  0.065545  0.022368  450.432458  192.751011  665.939788   \n",
       "19998  155310_372480  0.042145  0.015084  332.139261  146.760990  409.563611   \n",
       "19999  155311_453120  0.039820  0.016151  300.841059  117.395470  388.035180   \n",
       "\n",
       "           sb_std     sr_mean      sr_std   mfcc1_mean  ...  mfcc9_mean  \\\n",
       "0      267.139540  288.159666  267.139540   147.188060  ...   166.07562   \n",
       "1      135.532246  409.844515  135.532246  1282.769400  ...   765.03280   \n",
       "2      139.070343  719.222449  139.070343   292.690280  ...   214.49968   \n",
       "3       84.894649  391.783679   84.894649   188.996630  ...   146.75562   \n",
       "4      375.408621  766.763811  375.408621   196.932450  ...   172.74915   \n",
       "...           ...         ...         ...          ...  ...         ...   \n",
       "19995   63.090107  266.331118   63.090107   147.754410  ...    75.58570   \n",
       "19996   53.390921  101.288169   53.390921   631.029240  ...   866.60800   \n",
       "19997  285.751704  665.939788  285.751704   100.515335  ...    87.85434   \n",
       "19998  169.775745  409.563611  169.775745   144.219150  ...   139.86327   \n",
       "19999  205.422401  388.035180  205.422401   254.896030  ...   273.97130   \n",
       "\n",
       "        mfcc9std  mfcc10_mean   mfcc10std  mfcc11_mean  mfcc11std  \\\n",
       "0      216.15845   158.768600  209.785170   151.586730  203.43971   \n",
       "1      495.68510   590.328800  558.758240   425.313660  616.81950   \n",
       "2      316.38672   198.657150  314.673430   186.793670  313.87470   \n",
       "3      140.59268   130.843980  137.768260   116.143745  135.27222   \n",
       "4      268.45245   166.110350  266.885220   159.133510  265.19022   \n",
       "...          ...          ...         ...          ...        ...   \n",
       "19995   66.08795    52.333115   52.021423    29.923525   44.92963   \n",
       "19996  485.64580   861.181340  485.591300   855.655100  485.54526   \n",
       "19997  141.25677    81.044754  138.202670    74.912180  134.90385   \n",
       "19998  216.60248   130.948000  216.237840   122.950190  216.05978   \n",
       "19999  215.41476   259.883360  210.476330   245.089900  205.54100   \n",
       "\n",
       "       mfcc12_mean  mfcc12std  mfcc13_mean  mfcc13std  \n",
       "0       144.200240  197.13676   136.577680  190.80775  \n",
       "1       274.802150  663.55350   142.181600  694.35410  \n",
       "2       177.732700  313.87650   170.007640  313.99084  \n",
       "3       102.521700  133.01720    89.947320  130.88220  \n",
       "4       151.693700  263.76523   143.998100  262.70773  \n",
       "...            ...        ...          ...        ...  \n",
       "19995     8.635664   46.96052   -11.322803   55.88632  \n",
       "19996   850.119600  485.50488   844.646670  485.46692  \n",
       "19997    69.371315  131.49872    64.451385  128.12172  \n",
       "19998   116.407776  216.06279   111.108376  216.22318  \n",
       "19999   230.410980  201.05861   216.157270  197.14174  \n",
       "\n",
       "[20000 rows x 35 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=pd.read_csv('features.csv')\n",
    "\n",
    "# Let's have a look at the data\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the available features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sample_key', 'zcr_mean', 'zcr_std', 'sc_mean', 'sc_std', 'sb_mean',\n",
       "       'sb_std', 'sr_mean', 'sr_std', 'mfcc1_mean', 'mfcc1std', 'mfcc2_mean',\n",
       "       'mfcc2std', 'mfcc3_mean', 'mfcc3std', 'mfcc4_mean', 'mfcc4std',\n",
       "       'mfcc5_mean', 'mfcc5std', 'mfcc6_mean', 'mfcc6std', 'mfcc7_mean',\n",
       "       'mfcc7std', 'mfcc8_mean', 'mfcc8std', 'mfcc9_mean', 'mfcc9std',\n",
       "       'mfcc10_mean', 'mfcc10std', 'mfcc11_mean', 'mfcc11std', 'mfcc12_mean',\n",
       "       'mfcc12std', 'mfcc13_mean', 'mfcc13std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we swap the VGGish features with our own features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into well-balanced, predefined train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      000178_3840\n",
       "1     000308_61440\n",
       "2    000312_184320\n",
       "3    000319_145920\n",
       "4    000321_218880\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_train = pd.read_csv(\n",
    "    os.path.join(DATA_ROOT, \"partitions/split01_train.csv\"), header=None,\n",
    ").squeeze(\"columns\")\n",
    "split_test = pd.read_csv(\n",
    "    os.path.join(DATA_ROOT, \"partitions/split01_test.csv\"), header=None,\n",
    ").squeeze(\"columns\")\n",
    "\n",
    "split_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train: 14915,  # Test: 5085\n"
     ]
    }
   ],
   "source": [
    "print('# Train: {},  # Test: {}'.format(len(split_train), len(split_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = set(split_train)\n",
    "test_set = set(split_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These loops go through all sample keys, and save their row numbers\n",
    "# to either idx_train or idx_test\n",
    "#\n",
    "# This will be useful in the next step for slicing the array data\n",
    "idx_train, idx_test = [], []\n",
    "\n",
    "for idx, n in enumerate(sample_key):\n",
    "    if n in train_set:\n",
    "        idx_train.append(idx)\n",
    "    elif n in test_set:\n",
    "        idx_test.append(idx)\n",
    "    else:\n",
    "        # This should never happen, but better safe than sorry.\n",
    "        raise RuntimeError('Unknown sample key={}! Abort!'.format(sample_key[n]))\n",
    "        \n",
    "# Finally, cast the idx_* arrays to numpy structures\n",
    "idx_train = np.asarray(idx_train)\n",
    "idx_test = np.asarray(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we use the split indices to partition the features, labels, and masks\n",
    "X_train = X[idx_train]\n",
    "X_test = X[idx_test]\n",
    "\n",
    "Y_true_train = Y_true[idx_train]\n",
    "Y_true_test = Y_true[idx_test]\n",
    "\n",
    "Y_mask_train = Y_mask[idx_train]\n",
    "Y_mask_test = Y_mask[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14915, 33)\n",
      "(5085, 33)\n"
     ]
    }
   ],
   "source": [
    "# Print out the sliced shapes as a sanity check\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_ROOT, 'class-map.json'), 'r') as f:\n",
    "    class_map = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "In [MIC](https://arxiv.org/abs/1912.02606) they use the IRMAS dataset, which has one predominant instrument per sound file. The OpenMIC-2018 dataset has multiple instruments per sound file. This means that the OpenMIC-2018 dataset is a multi-label classification problem, while the IRMAS dataset is a multi-class classification problem. We didn't really understand the difference when we chose this project, and have since then learned that both problems have their own challenges.\n",
    "\n",
    "We work around this by using a one-vs-rest approach. This means that we train a binary classifier for each instrument. The classifier predicts whether the instrument is present in the audio file or not. We then use the predictions from all classifiers to predict the instruments in the audio file.\n",
    "\n",
    "In [MIC](https://arxiv.org/abs/1912.02606) they train six different models from [scikit-learn](https://scikit-learn.org/stable/):\n",
    "\n",
    "- *SVM* (Support Vector Machine)\n",
    "- *RF* (Random Forest)\n",
    "- *LR* (Logistic Regression)\n",
    "- *Decision Tree*\n",
    "- *XGBoost* (eXtreme Gradient Boosting)\n",
    "- *LGBM* (Light Gradient Boosting Machine)\n",
    "\n",
    "To evalute their performance, they use the following metrics:\n",
    "\n",
    "- *Precision*: $ \\frac{tp}{tp + fp} $\n",
    "- *Recall*: $ \\frac{tp}{tp + fn} $\n",
    "- *F1-score* (harmonic mean of precision and recall): $ \\frac{2 \\times precision \\times recall}{precision + recall} $\n",
    "- *Accuracy*: the number of correct predictions divided by the total number of predictions\n",
    "\n",
    "Since the OPENMIC dataset is an ongoing project, and still acquiring annotations, we found it a bit hard to mold the data. Out of convienence, we are still using the approach in [modelling baseline](https://github.com/cosmir/openmic-2018/blob/master/examples/modeling-baseline.ipynb), as suggested by the authors of the dataset. \n",
    "\n",
    "We are replicating the method of [MIC](https://arxiv.org/abs/1912.02606), and so, only looking at six instrument:\n",
    "\n",
    "```python\n",
    "    ['flute', 'guitar', 'organ', 'piano', 'trumpet', 'voice']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = ['accordion', 'banjo', 'bass', 'cello', 'clarinet',\n",
    "       'cymbals', 'drums', 'flute', 'guitar', 'mallet_percussion', 'mandolin',\n",
    "       'organ', 'piano', 'saxophone', 'synthesizer', 'trombone', 'trumpet',\n",
    "       'ukulele', 'violin', 'voice']\n",
    "\n",
    "MIC_instruments = ['flute', 'guitar', 'organ', 'piano', 'trumpet', 'voice']\n",
    "\n",
    "features = ['zcr_mean', 'zcr_std', 'sc_mean',\n",
    "       'sc_std', 'sb_mean', 'sb_std', 'sr_mean', 'sr_std', 'mfcc1_mean',\n",
    "       'mfcc1std', 'mfcc2_mean', 'mfcc2std', 'mfcc3_mean', 'mfcc3std',\n",
    "       'mfcc4_mean', 'mfcc4std', 'mfcc5_mean', 'mfcc5std', 'mfcc6_mean',\n",
    "       'mfcc6std', 'mfcc7_mean', 'mfcc7std', 'mfcc8_mean', 'mfcc8std',\n",
    "       'mfcc9_mean', 'mfcc9std', 'mfcc10_mean', 'mfcc10std', 'mfcc11_mean',\n",
    "       'mfcc11std', 'mfcc12_mean', 'mfcc12std', 'mfcc13_mean', 'mfcc13std']\n",
    "\n",
    "MIC_features = ['zcr_mean', 'sc_mean', 'sb_mean', 'sr_mean', 'mfcc1_mean',\n",
    "       'mfcc2_mean', 'mfcc3_mean', 'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean',\n",
    "       'mfcc7_mean', 'mfcc8_mean', 'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean',\n",
    "       'mfcc12_mean', 'mfcc13_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the choice to replicate the article features and instruments, or picking some additional features we used.\n",
    "Our baseline is the article features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = MIC_features\n",
    "#FEATURES = features\n",
    "INSTRUMENTS = MIC_instruments\n",
    "#INSTRUMENTS = instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling baseline\n",
    "\n",
    "This is the heart of the [modelling baseline](https://github.com/cosmir/openmic-2018/blob/master/examples/modeling-baseline.ipynb). As we are training six models, we turned it into a method that returns accuracy, classifier and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,balanced_accuracy_score,precision_score,recall_score\n",
    "\n",
    "def modelling_baseline(X_train, X_test, Y_true_train, Y_true_test, Y_mask_train, Y_mask_test, class_map, clf, instruments, output_dict=True):\n",
    "    \"\"\"\n",
    "    Use the modeling baseline to train a classifier for each instrument\n",
    "    Returns randomforrest_models - classifiers for trained model\n",
    "            reports - classification reports for each instrument\n",
    "            accuracies - accuracies for each instrument\n",
    "    \"\"\"\n",
    "    # This dictionary will include the classifiers for each model\n",
    "    models = dict()\n",
    "    reports = dict()\n",
    "    accuracies = dict()\n",
    "\n",
    "\n",
    "    # We'll iterate over all istrument classes, and fit a model for each one\n",
    "    # After training, we'll print a classification report for each instrument\n",
    "    for instrument in instruments:\n",
    "\n",
    "        # Map the instrument name to its column number\n",
    "        inst_num = class_map[instrument]\n",
    "\n",
    "        # Step 1: sub-sample the data\n",
    "\n",
    "        # First, we need to select down to the data for which we have annotations\n",
    "        # This is what the mask arrays are for\n",
    "        train_inst = Y_mask_train[:, inst_num]\n",
    "        test_inst = Y_mask_test[:, inst_num]\n",
    "\n",
    "        # Here, we're using the Y_mask_train array to slice out only the training examples\n",
    "        # for which we have annotations for the given class\n",
    "        X_train_inst = X_train[train_inst]\n",
    "\n",
    "        # Again, we slice the labels to the annotated examples\n",
    "        # We thresold the label likelihoods at 0.5 to get binary labels\n",
    "        Y_true_train_inst = Y_true_train[train_inst, inst_num] >= 0.5\n",
    "\n",
    "        # Repeat the above slicing and dicing but for the test set\n",
    "        X_test_inst = X_test[test_inst]\n",
    "        Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "        # Step 3.\n",
    "        # Initialize a new classifier\n",
    "        # No\n",
    "\n",
    "        # Step 4.\n",
    "        clf.fit(X_train_inst, Y_true_train_inst)\n",
    "\n",
    "        # Step 5.\n",
    "        # Finally, evaluate the model on both test data\n",
    "        Y_pred_test = clf.predict(X_test_inst)\n",
    "        \n",
    "        # Store the classifier in our dictionary\n",
    "        models[instrument] = clf\n",
    "        reports[instrument] = classification_report(Y_true_test_inst, Y_pred_test, output_dict=output_dict)\n",
    "        accuracies[instrument] = balanced_accuracy_score(Y_true_test_inst, Y_pred_test)\n",
    "\n",
    "    return models, reports, accuracies\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, if you want to find the result of a the *Random Forest* model on *flute* instrument, you can do:\n",
    "\n",
    "\n",
    "```python\n",
    "    >>> results['RandomForest']['Accuracy']['flute']\n",
    "    0.43333333\n",
    "```\n",
    "\n",
    "Here we set up final `results` dictionary to store all the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\"model\": None, \"report\": None, \"accuracy\": None}\n",
    "models = ['RandomForest', 'SVM', 'LogisticRegression', 'GradientBoost', 'DecisionTree', 'LGBM']\n",
    "results = dict()\n",
    "\n",
    "for model in models:\n",
    "    results[model] = dict(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "From here, we call the previously defined `modelling_baseline` method to train all six classifiers. MIC doesn't mention any use of standardizing, but we add a `StandardScaler()` where we think it makes sense to use it. The results are stored in the `results` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clfs = {\"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"SVM\": make_pipeline(StandardScaler(), SVC()),\n",
    "        \"LogisticRegression\": make_pipeline(StandardScaler(), LogisticRegression(max_iter = 1000)),\n",
    "        \"GradientBoost\": GradientBoostingClassifier(learning_rate=0.3),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(),\n",
    "        \"LGBM\": LGBMClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model, clf in clfs.items():\n",
    "    # a quite nasty one-liner, but it does the job Â¯\\_(ãƒ„)_/Â¯\n",
    "    results[model][\"model\"], results[model][\"report\"], results[model][\"accuracy\"] = modelling_baseline(\n",
    "                                                    X_train, \n",
    "                                                    X_test, \n",
    "                                                    Y_true_train, \n",
    "                                                    Y_true_test, \n",
    "                                                    Y_mask_train, \n",
    "                                                    Y_mask_test, \n",
    "                                                    class_map, \n",
    "                                                    clf, \n",
    "                                                    INSTRUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We are going to present and compare our metrics to the once in MIC. Too much time was spent on pivoting the Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27496/2375201178.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  df_tex = df_pivot.to_latex(float_format=\"%.2f\",\n"
     ]
    }
   ],
   "source": [
    "article_columns = ['P', 'R', 'F1', 'support']\n",
    "\n",
    "columns = [\"precision\",\t\"recall\", \"f1-score\", \"support\", \"model\", \"instrument\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df = df.rename(columns={ \"precision\": \"P\", \n",
    "                    \"recall\": \"R\", \n",
    "                    \"f1-score\": \"F1\"\n",
    "                    }, inplace=True)\n",
    "\n",
    "for instrument in INSTRUMENTS:\n",
    "    for model in models:\n",
    "        di = results[model][\"report\"][instrument]['True']\n",
    "        di['model'] = model\n",
    "        di['instrument'] = instrument\n",
    "        df = pd.concat([df, pd.DataFrame(di, index=[0]).rename(columns={ \"precision\": \"P\", \n",
    "                    \"recall\": \"R\", \n",
    "                    \"f1-score\": \"F1\",\n",
    "                    })], ignore_index=True)\n",
    "\n",
    "n_rows, n_cols = df.shape\n",
    "df_melted = df.melt(id_vars=[\"model\", \"instrument\"], var_name=\"metric\", value_name=\"value\")\n",
    "\n",
    "df_pivot = df_melted.pivot_table(columns=['model', \"metric\"], \n",
    "                        index=['instrument'], \n",
    "                        values=['value'], \n",
    "                        aggfunc='mean'\n",
    "                        )\n",
    "df_tex = df_pivot.to_latex(float_format=\"%.2f\", \n",
    "                            multicolumn=True, \n",
    "                            multirow=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present our result as a heatmap, with the f1-score (F1), precision (P), and recall (R), of each model on each instrument. We also add `support` to get a sense of how many samples contains. It is hard to tell which model is the better, but we can see that piano, voice and guitar does well over all models. Trumpet and organ does not do as well, and flute is the worst performing instrument. Compared to MICs table, there is a lot of variation in the results. Piano is doing much better than MIC, something which could be the result of many potential factors. First, having a predominant instrument per sound file, and having multiple instruments per sound file, might make a big difference for their suggested approach. To get to the bottom of this, we would need to do a more in-depth analysis of the data, which is outside the scope of this project. Second, it could be a sign of overfitting.\n",
    "\n",
    "> **NOTE**: as of now, the SVM model is not working for flute. We are not sure why, but we are looking into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_95659_row0_col0, #T_95659_row0_col8, #T_95659_row2_col16 {\n",
       "  background-color: #feefd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row0_col1 {\n",
       "  background-color: #fff4e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row0_col2 {\n",
       "  background-color: #fee9ca;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row0_col3, #T_95659_row0_col7, #T_95659_row0_col11, #T_95659_row0_col15, #T_95659_row0_col19, #T_95659_row0_col23 {\n",
       "  background-color: #fdcf99;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row0_col4, #T_95659_row0_col5, #T_95659_row0_col6, #T_95659_row0_col12, #T_95659_row0_col13, #T_95659_row0_col14, #T_95659_row0_col16, #T_95659_row0_col18, #T_95659_row0_col20, #T_95659_row0_col21, #T_95659_row0_col22, #T_95659_row2_col0, #T_95659_row2_col1, #T_95659_row2_col2, #T_95659_row2_col3, #T_95659_row2_col7, #T_95659_row2_col8, #T_95659_row2_col9, #T_95659_row2_col10, #T_95659_row2_col11, #T_95659_row2_col15, #T_95659_row2_col17, #T_95659_row2_col19, #T_95659_row2_col23 {\n",
       "  background-color: #fff7ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row0_col9 {\n",
       "  background-color: #fddaab;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row0_col10 {\n",
       "  background-color: #fff6ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row0_col17 {\n",
       "  background-color: #fee9cb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row1_col0 {\n",
       "  background-color: #c4170f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col1 {\n",
       "  background-color: #d32a1b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col2, #T_95659_row5_col20 {\n",
       "  background-color: #b50302;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col3, #T_95659_row1_col7, #T_95659_row1_col11, #T_95659_row1_col15, #T_95659_row1_col19, #T_95659_row1_col23 {\n",
       "  background-color: #bd0e09;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col4 {\n",
       "  background-color: #ba0906;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col5 {\n",
       "  background-color: #dc3c28;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col6, #T_95659_row5_col14 {\n",
       "  background-color: #960000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col8 {\n",
       "  background-color: #b90805;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col9 {\n",
       "  background-color: #ce2417;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col10 {\n",
       "  background-color: #a10000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col12 {\n",
       "  background-color: #ab0000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col13 {\n",
       "  background-color: #dd3e2a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col14 {\n",
       "  background-color: #820000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col16 {\n",
       "  background-color: #b20000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col17 {\n",
       "  background-color: #e24933;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col18, #T_95659_row5_col18 {\n",
       "  background-color: #840000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col20 {\n",
       "  background-color: #a30000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col21, #T_95659_row3_col3, #T_95659_row3_col7, #T_95659_row3_col11, #T_95659_row3_col15, #T_95659_row3_col19, #T_95659_row3_col23, #T_95659_row5_col4, #T_95659_row5_col8 {\n",
       "  background-color: #be0f0a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row1_col22, #T_95659_row3_col0, #T_95659_row3_col1, #T_95659_row3_col2, #T_95659_row3_col4, #T_95659_row3_col5, #T_95659_row3_col6, #T_95659_row3_col8, #T_95659_row3_col9, #T_95659_row3_col10, #T_95659_row3_col12, #T_95659_row3_col13, #T_95659_row3_col14, #T_95659_row3_col16, #T_95659_row3_col17, #T_95659_row3_col18, #T_95659_row3_col20, #T_95659_row3_col21, #T_95659_row4_col3, #T_95659_row4_col7, #T_95659_row4_col11, #T_95659_row4_col15, #T_95659_row4_col19, #T_95659_row4_col23 {\n",
       "  background-color: #7f0000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row2_col4 {\n",
       "  background-color: #fff0dc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col5 {\n",
       "  background-color: #fff1de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col6 {\n",
       "  background-color: #fff1dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col12 {\n",
       "  background-color: #fdbb85;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col13 {\n",
       "  background-color: #fdc892;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col14 {\n",
       "  background-color: #fdcc96;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col18 {\n",
       "  background-color: #feedd4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col20 {\n",
       "  background-color: #fdad77;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row2_col21 {\n",
       "  background-color: #f3724e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row2_col22 {\n",
       "  background-color: #fdc791;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row3_col22 {\n",
       "  background-color: #810000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row4_col0, #T_95659_row4_col8, #T_95659_row4_col10 {\n",
       "  background-color: #fdc68f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col1 {\n",
       "  background-color: #fdd19b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col2 {\n",
       "  background-color: #fdb982;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col4 {\n",
       "  background-color: #fdca94;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col5 {\n",
       "  background-color: #fdd9aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col6 {\n",
       "  background-color: #fdc690;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col9 {\n",
       "  background-color: #fdc992;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col12 {\n",
       "  background-color: #fdc28b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col13 {\n",
       "  background-color: #fdb881;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col14 {\n",
       "  background-color: #fdd49f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col16 {\n",
       "  background-color: #fdd39d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col17 {\n",
       "  background-color: #feefda;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col18 {\n",
       "  background-color: #fdcb95;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row4_col20 {\n",
       "  background-color: #fb8b58;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row4_col21 {\n",
       "  background-color: #ea5a3f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row4_col22 {\n",
       "  background-color: #fdaf79;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95659_row5_col0 {\n",
       "  background-color: #d83221;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col1 {\n",
       "  background-color: #ec5d42;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col2 {\n",
       "  background-color: #b80604;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col3, #T_95659_row5_col7, #T_95659_row5_col11, #T_95659_row5_col15, #T_95659_row5_col19, #T_95659_row5_col23 {\n",
       "  background-color: #fa8656;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col5 {\n",
       "  background-color: #e8553c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col6 {\n",
       "  background-color: #8c0000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col9 {\n",
       "  background-color: #e44d35;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col10 {\n",
       "  background-color: #870000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col12 {\n",
       "  background-color: #b40201;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col13 {\n",
       "  background-color: #df412c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col16 {\n",
       "  background-color: #c61a11;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col17 {\n",
       "  background-color: #f98455;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col21 {\n",
       "  background-color: #cb2015;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95659_row5_col22 {\n",
       "  background-color: #930000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_95659\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_95659_level0_col0\" class=\"col_heading level0 col0\" colspan=\"24\">value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >model</th>\n",
       "      <th id=\"T_95659_level1_col0\" class=\"col_heading level1 col0\" colspan=\"4\">DecisionTree</th>\n",
       "      <th id=\"T_95659_level1_col4\" class=\"col_heading level1 col4\" colspan=\"4\">GradientBoost</th>\n",
       "      <th id=\"T_95659_level1_col8\" class=\"col_heading level1 col8\" colspan=\"4\">LGBM</th>\n",
       "      <th id=\"T_95659_level1_col12\" class=\"col_heading level1 col12\" colspan=\"4\">LogisticRegression</th>\n",
       "      <th id=\"T_95659_level1_col16\" class=\"col_heading level1 col16\" colspan=\"4\">RandomForest</th>\n",
       "      <th id=\"T_95659_level1_col20\" class=\"col_heading level1 col20\" colspan=\"4\">SVM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level2\" >metric</th>\n",
       "      <th id=\"T_95659_level2_col0\" class=\"col_heading level2 col0\" >F1</th>\n",
       "      <th id=\"T_95659_level2_col1\" class=\"col_heading level2 col1\" >P</th>\n",
       "      <th id=\"T_95659_level2_col2\" class=\"col_heading level2 col2\" >R</th>\n",
       "      <th id=\"T_95659_level2_col3\" class=\"col_heading level2 col3\" >support</th>\n",
       "      <th id=\"T_95659_level2_col4\" class=\"col_heading level2 col4\" >F1</th>\n",
       "      <th id=\"T_95659_level2_col5\" class=\"col_heading level2 col5\" >P</th>\n",
       "      <th id=\"T_95659_level2_col6\" class=\"col_heading level2 col6\" >R</th>\n",
       "      <th id=\"T_95659_level2_col7\" class=\"col_heading level2 col7\" >support</th>\n",
       "      <th id=\"T_95659_level2_col8\" class=\"col_heading level2 col8\" >F1</th>\n",
       "      <th id=\"T_95659_level2_col9\" class=\"col_heading level2 col9\" >P</th>\n",
       "      <th id=\"T_95659_level2_col10\" class=\"col_heading level2 col10\" >R</th>\n",
       "      <th id=\"T_95659_level2_col11\" class=\"col_heading level2 col11\" >support</th>\n",
       "      <th id=\"T_95659_level2_col12\" class=\"col_heading level2 col12\" >F1</th>\n",
       "      <th id=\"T_95659_level2_col13\" class=\"col_heading level2 col13\" >P</th>\n",
       "      <th id=\"T_95659_level2_col14\" class=\"col_heading level2 col14\" >R</th>\n",
       "      <th id=\"T_95659_level2_col15\" class=\"col_heading level2 col15\" >support</th>\n",
       "      <th id=\"T_95659_level2_col16\" class=\"col_heading level2 col16\" >F1</th>\n",
       "      <th id=\"T_95659_level2_col17\" class=\"col_heading level2 col17\" >P</th>\n",
       "      <th id=\"T_95659_level2_col18\" class=\"col_heading level2 col18\" >R</th>\n",
       "      <th id=\"T_95659_level2_col19\" class=\"col_heading level2 col19\" >support</th>\n",
       "      <th id=\"T_95659_level2_col20\" class=\"col_heading level2 col20\" >F1</th>\n",
       "      <th id=\"T_95659_level2_col21\" class=\"col_heading level2 col21\" >P</th>\n",
       "      <th id=\"T_95659_level2_col22\" class=\"col_heading level2 col22\" >R</th>\n",
       "      <th id=\"T_95659_level2_col23\" class=\"col_heading level2 col23\" >support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >instrument</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "      <th class=\"blank col16\" >&nbsp;</th>\n",
       "      <th class=\"blank col17\" >&nbsp;</th>\n",
       "      <th class=\"blank col18\" >&nbsp;</th>\n",
       "      <th class=\"blank col19\" >&nbsp;</th>\n",
       "      <th class=\"blank col20\" >&nbsp;</th>\n",
       "      <th class=\"blank col21\" >&nbsp;</th>\n",
       "      <th class=\"blank col22\" >&nbsp;</th>\n",
       "      <th class=\"blank col23\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_95659_level0_row0\" class=\"row_heading level0 row0\" >flute</th>\n",
       "      <td id=\"T_95659_row0_col0\" class=\"data row0 col0\" >0.37</td>\n",
       "      <td id=\"T_95659_row0_col1\" class=\"data row0 col1\" >0.36</td>\n",
       "      <td id=\"T_95659_row0_col2\" class=\"data row0 col2\" >0.39</td>\n",
       "      <td id=\"T_95659_row0_col3\" class=\"data row0 col3\" >175.00</td>\n",
       "      <td id=\"T_95659_row0_col4\" class=\"data row0 col4\" >0.34</td>\n",
       "      <td id=\"T_95659_row0_col5\" class=\"data row0 col5\" >0.44</td>\n",
       "      <td id=\"T_95659_row0_col6\" class=\"data row0 col6\" >0.28</td>\n",
       "      <td id=\"T_95659_row0_col7\" class=\"data row0 col7\" >175.00</td>\n",
       "      <td id=\"T_95659_row0_col8\" class=\"data row0 col8\" >0.38</td>\n",
       "      <td id=\"T_95659_row0_col9\" class=\"data row0 col9\" >0.50</td>\n",
       "      <td id=\"T_95659_row0_col10\" class=\"data row0 col10\" >0.30</td>\n",
       "      <td id=\"T_95659_row0_col11\" class=\"data row0 col11\" >175.00</td>\n",
       "      <td id=\"T_95659_row0_col12\" class=\"data row0 col12\" >0.07</td>\n",
       "      <td id=\"T_95659_row0_col13\" class=\"data row0 col13\" >0.35</td>\n",
       "      <td id=\"T_95659_row0_col14\" class=\"data row0 col14\" >0.04</td>\n",
       "      <td id=\"T_95659_row0_col15\" class=\"data row0 col15\" >175.00</td>\n",
       "      <td id=\"T_95659_row0_col16\" class=\"data row0 col16\" >0.33</td>\n",
       "      <td id=\"T_95659_row0_col17\" class=\"data row0 col17\" >0.55</td>\n",
       "      <td id=\"T_95659_row0_col18\" class=\"data row0 col18\" >0.23</td>\n",
       "      <td id=\"T_95659_row0_col19\" class=\"data row0 col19\" >175.00</td>\n",
       "      <td id=\"T_95659_row0_col20\" class=\"data row0 col20\" >0.00</td>\n",
       "      <td id=\"T_95659_row0_col21\" class=\"data row0 col21\" >0.00</td>\n",
       "      <td id=\"T_95659_row0_col22\" class=\"data row0 col22\" >0.00</td>\n",
       "      <td id=\"T_95659_row0_col23\" class=\"data row0 col23\" >175.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95659_level0_row1\" class=\"row_heading level0 row1\" >guitar</th>\n",
       "      <td id=\"T_95659_row1_col0\" class=\"data row1 col0\" >0.76</td>\n",
       "      <td id=\"T_95659_row1_col1\" class=\"data row1 col1\" >0.75</td>\n",
       "      <td id=\"T_95659_row1_col2\" class=\"data row1 col2\" >0.76</td>\n",
       "      <td id=\"T_95659_row1_col3\" class=\"data row1 col3\" >286.00</td>\n",
       "      <td id=\"T_95659_row1_col4\" class=\"data row1 col4\" >0.82</td>\n",
       "      <td id=\"T_95659_row1_col5\" class=\"data row1 col5\" >0.77</td>\n",
       "      <td id=\"T_95659_row1_col6\" class=\"data row1 col6\" >0.87</td>\n",
       "      <td id=\"T_95659_row1_col7\" class=\"data row1 col7\" >286.00</td>\n",
       "      <td id=\"T_95659_row1_col8\" class=\"data row1 col8\" >0.82</td>\n",
       "      <td id=\"T_95659_row1_col9\" class=\"data row1 col9\" >0.78</td>\n",
       "      <td id=\"T_95659_row1_col10\" class=\"data row1 col10\" >0.87</td>\n",
       "      <td id=\"T_95659_row1_col11\" class=\"data row1 col11\" >286.00</td>\n",
       "      <td id=\"T_95659_row1_col12\" class=\"data row1 col12\" >0.82</td>\n",
       "      <td id=\"T_95659_row1_col13\" class=\"data row1 col13\" >0.73</td>\n",
       "      <td id=\"T_95659_row1_col14\" class=\"data row1 col14\" >0.94</td>\n",
       "      <td id=\"T_95659_row1_col15\" class=\"data row1 col15\" >286.00</td>\n",
       "      <td id=\"T_95659_row1_col16\" class=\"data row1 col16\" >0.83</td>\n",
       "      <td id=\"T_95659_row1_col17\" class=\"data row1 col17\" >0.78</td>\n",
       "      <td id=\"T_95659_row1_col18\" class=\"data row1 col18\" >0.89</td>\n",
       "      <td id=\"T_95659_row1_col19\" class=\"data row1 col19\" >286.00</td>\n",
       "      <td id=\"T_95659_row1_col20\" class=\"data row1 col20\" >0.83</td>\n",
       "      <td id=\"T_95659_row1_col21\" class=\"data row1 col21\" >0.74</td>\n",
       "      <td id=\"T_95659_row1_col22\" class=\"data row1 col22\" >0.95</td>\n",
       "      <td id=\"T_95659_row1_col23\" class=\"data row1 col23\" >286.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95659_level0_row2\" class=\"row_heading level0 row2\" >organ</th>\n",
       "      <td id=\"T_95659_row2_col0\" class=\"data row2 col0\" >0.34</td>\n",
       "      <td id=\"T_95659_row2_col1\" class=\"data row2 col1\" >0.35</td>\n",
       "      <td id=\"T_95659_row2_col2\" class=\"data row2 col2\" >0.33</td>\n",
       "      <td id=\"T_95659_row2_col3\" class=\"data row2 col3\" >121.00</td>\n",
       "      <td id=\"T_95659_row2_col4\" class=\"data row2 col4\" >0.37</td>\n",
       "      <td id=\"T_95659_row2_col5\" class=\"data row2 col5\" >0.46</td>\n",
       "      <td id=\"T_95659_row2_col6\" class=\"data row2 col6\" >0.31</td>\n",
       "      <td id=\"T_95659_row2_col7\" class=\"data row2 col7\" >121.00</td>\n",
       "      <td id=\"T_95659_row2_col8\" class=\"data row2 col8\" >0.34</td>\n",
       "      <td id=\"T_95659_row2_col9\" class=\"data row2 col9\" >0.40</td>\n",
       "      <td id=\"T_95659_row2_col10\" class=\"data row2 col10\" >0.30</td>\n",
       "      <td id=\"T_95659_row2_col11\" class=\"data row2 col11\" >121.00</td>\n",
       "      <td id=\"T_95659_row2_col12\" class=\"data row2 col12\" >0.38</td>\n",
       "      <td id=\"T_95659_row2_col13\" class=\"data row2 col13\" >0.51</td>\n",
       "      <td id=\"T_95659_row2_col14\" class=\"data row2 col14\" >0.31</td>\n",
       "      <td id=\"T_95659_row2_col15\" class=\"data row2 col15\" >121.00</td>\n",
       "      <td id=\"T_95659_row2_col16\" class=\"data row2 col16\" >0.37</td>\n",
       "      <td id=\"T_95659_row2_col17\" class=\"data row2 col17\" >0.50</td>\n",
       "      <td id=\"T_95659_row2_col18\" class=\"data row2 col18\" >0.29</td>\n",
       "      <td id=\"T_95659_row2_col19\" class=\"data row2 col19\" >121.00</td>\n",
       "      <td id=\"T_95659_row2_col20\" class=\"data row2 col20\" >0.38</td>\n",
       "      <td id=\"T_95659_row2_col21\" class=\"data row2 col21\" >0.51</td>\n",
       "      <td id=\"T_95659_row2_col22\" class=\"data row2 col22\" >0.30</td>\n",
       "      <td id=\"T_95659_row2_col23\" class=\"data row2 col23\" >121.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95659_level0_row3\" class=\"row_heading level0 row3\" >piano</th>\n",
       "      <td id=\"T_95659_row3_col0\" class=\"data row3 col0\" >0.85</td>\n",
       "      <td id=\"T_95659_row3_col1\" class=\"data row3 col1\" >0.88</td>\n",
       "      <td id=\"T_95659_row3_col2\" class=\"data row3 col2\" >0.82</td>\n",
       "      <td id=\"T_95659_row3_col3\" class=\"data row3 col3\" >285.00</td>\n",
       "      <td id=\"T_95659_row3_col4\" class=\"data row3 col4\" >0.90</td>\n",
       "      <td id=\"T_95659_row3_col5\" class=\"data row3 col5\" >0.90</td>\n",
       "      <td id=\"T_95659_row3_col6\" class=\"data row3 col6\" >0.91</td>\n",
       "      <td id=\"T_95659_row3_col7\" class=\"data row3 col7\" >285.00</td>\n",
       "      <td id=\"T_95659_row3_col8\" class=\"data row3 col8\" >0.90</td>\n",
       "      <td id=\"T_95659_row3_col9\" class=\"data row3 col9\" >0.89</td>\n",
       "      <td id=\"T_95659_row3_col10\" class=\"data row3 col10\" >0.92</td>\n",
       "      <td id=\"T_95659_row3_col11\" class=\"data row3 col11\" >285.00</td>\n",
       "      <td id=\"T_95659_row3_col12\" class=\"data row3 col12\" >0.91</td>\n",
       "      <td id=\"T_95659_row3_col13\" class=\"data row3 col13\" >0.88</td>\n",
       "      <td id=\"T_95659_row3_col14\" class=\"data row3 col14\" >0.95</td>\n",
       "      <td id=\"T_95659_row3_col15\" class=\"data row3 col15\" >285.00</td>\n",
       "      <td id=\"T_95659_row3_col16\" class=\"data row3 col16\" >0.90</td>\n",
       "      <td id=\"T_95659_row3_col17\" class=\"data row3 col17\" >0.91</td>\n",
       "      <td id=\"T_95659_row3_col18\" class=\"data row3 col18\" >0.90</td>\n",
       "      <td id=\"T_95659_row3_col19\" class=\"data row3 col19\" >285.00</td>\n",
       "      <td id=\"T_95659_row3_col20\" class=\"data row3 col20\" >0.91</td>\n",
       "      <td id=\"T_95659_row3_col21\" class=\"data row3 col21\" >0.88</td>\n",
       "      <td id=\"T_95659_row3_col22\" class=\"data row3 col22\" >0.94</td>\n",
       "      <td id=\"T_95659_row3_col23\" class=\"data row3 col23\" >285.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95659_level0_row4\" class=\"row_heading level0 row4\" >trumpet</th>\n",
       "      <td id=\"T_95659_row4_col0\" class=\"data row4 col0\" >0.50</td>\n",
       "      <td id=\"T_95659_row4_col1\" class=\"data row4 col1\" >0.49</td>\n",
       "      <td id=\"T_95659_row4_col2\" class=\"data row4 col2\" >0.52</td>\n",
       "      <td id=\"T_95659_row4_col3\" class=\"data row4 col3\" >318.00</td>\n",
       "      <td id=\"T_95659_row4_col4\" class=\"data row4 col4\" >0.51</td>\n",
       "      <td id=\"T_95659_row4_col5\" class=\"data row4 col5\" >0.54</td>\n",
       "      <td id=\"T_95659_row4_col6\" class=\"data row4 col6\" >0.48</td>\n",
       "      <td id=\"T_95659_row4_col7\" class=\"data row4 col7\" >318.00</td>\n",
       "      <td id=\"T_95659_row4_col8\" class=\"data row4 col8\" >0.52</td>\n",
       "      <td id=\"T_95659_row4_col9\" class=\"data row4 col9\" >0.54</td>\n",
       "      <td id=\"T_95659_row4_col10\" class=\"data row4 col10\" >0.50</td>\n",
       "      <td id=\"T_95659_row4_col11\" class=\"data row4 col11\" >318.00</td>\n",
       "      <td id=\"T_95659_row4_col12\" class=\"data row4 col12\" >0.36</td>\n",
       "      <td id=\"T_95659_row4_col13\" class=\"data row4 col13\" >0.55</td>\n",
       "      <td id=\"T_95659_row4_col14\" class=\"data row4 col14\" >0.26</td>\n",
       "      <td id=\"T_95659_row4_col15\" class=\"data row4 col15\" >318.00</td>\n",
       "      <td id=\"T_95659_row4_col16\" class=\"data row4 col16\" >0.47</td>\n",
       "      <td id=\"T_95659_row4_col17\" class=\"data row4 col17\" >0.53</td>\n",
       "      <td id=\"T_95659_row4_col18\" class=\"data row4 col18\" >0.43</td>\n",
       "      <td id=\"T_95659_row4_col19\" class=\"data row4 col19\" >318.00</td>\n",
       "      <td id=\"T_95659_row4_col20\" class=\"data row4 col20\" >0.46</td>\n",
       "      <td id=\"T_95659_row4_col21\" class=\"data row4 col21\" >0.57</td>\n",
       "      <td id=\"T_95659_row4_col22\" class=\"data row4 col22\" >0.39</td>\n",
       "      <td id=\"T_95659_row4_col23\" class=\"data row4 col23\" >318.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95659_level0_row5\" class=\"row_heading level0 row5\" >voice</th>\n",
       "      <td id=\"T_95659_row5_col0\" class=\"data row5 col0\" >0.72</td>\n",
       "      <td id=\"T_95659_row5_col1\" class=\"data row5 col1\" >0.69</td>\n",
       "      <td id=\"T_95659_row5_col2\" class=\"data row5 col2\" >0.75</td>\n",
       "      <td id=\"T_95659_row5_col3\" class=\"data row5 col3\" >224.00</td>\n",
       "      <td id=\"T_95659_row5_col4\" class=\"data row5 col4\" >0.81</td>\n",
       "      <td id=\"T_95659_row5_col5\" class=\"data row5 col5\" >0.74</td>\n",
       "      <td id=\"T_95659_row5_col6\" class=\"data row5 col6\" >0.89</td>\n",
       "      <td id=\"T_95659_row5_col7\" class=\"data row5 col7\" >224.00</td>\n",
       "      <td id=\"T_95659_row5_col8\" class=\"data row5 col8\" >0.81</td>\n",
       "      <td id=\"T_95659_row5_col9\" class=\"data row5 col9\" >0.73</td>\n",
       "      <td id=\"T_95659_row5_col10\" class=\"data row5 col10\" >0.91</td>\n",
       "      <td id=\"T_95659_row5_col11\" class=\"data row5 col11\" >224.00</td>\n",
       "      <td id=\"T_95659_row5_col12\" class=\"data row5 col12\" >0.80</td>\n",
       "      <td id=\"T_95659_row5_col13\" class=\"data row5 col13\" >0.72</td>\n",
       "      <td id=\"T_95659_row5_col14\" class=\"data row5 col14\" >0.90</td>\n",
       "      <td id=\"T_95659_row5_col15\" class=\"data row5 col15\" >224.00</td>\n",
       "      <td id=\"T_95659_row5_col16\" class=\"data row5 col16\" >0.79</td>\n",
       "      <td id=\"T_95659_row5_col17\" class=\"data row5 col17\" >0.72</td>\n",
       "      <td id=\"T_95659_row5_col18\" class=\"data row5 col18\" >0.89</td>\n",
       "      <td id=\"T_95659_row5_col19\" class=\"data row5 col19\" >224.00</td>\n",
       "      <td id=\"T_95659_row5_col20\" class=\"data row5 col20\" >0.79</td>\n",
       "      <td id=\"T_95659_row5_col21\" class=\"data row5 col21\" >0.70</td>\n",
       "      <td id=\"T_95659_row5_col22\" class=\"data row5 col22\" >0.91</td>\n",
       "      <td id=\"T_95659_row5_col23\" class=\"data row5 col23\" >224.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff4b053940>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot[df_pivot.columns[0:]].style.format(\"{:.2f}\").background_gradient(cmap='OrRd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table II from MIC:\n",
    "\n",
    "<img src=\"../imgs/MIC-table-ii.png\" alt=\"MIC Table II\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compare the accuracy of our models to the ones in MIC. As we have one binary classifier for each instrument, we take the mean and standard deviation of each classifier and model. It is clear that SVM was the best for MIC. Our best model is the XGBoost model, with a mean accuracy of 0.67 Â± 0.10. As the datsets are quite different, it is wise to take this comparison with a grain of salt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_87375_row0_col0, #T_87375_row0_col1 {\n",
       "  background-color: #fff7ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_87375_row1_col0, #T_87375_row3_col1 {\n",
       "  background-color: #7f0000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_87375_row1_col1 {\n",
       "  background-color: #f88053;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_87375_row2_col0 {\n",
       "  background-color: #8c0000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_87375_row2_col1 {\n",
       "  background-color: #fdc089;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_87375_row3_col0 {\n",
       "  background-color: #fdbe87;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_87375_row4_col0 {\n",
       "  background-color: #860000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_87375_row4_col1 {\n",
       "  background-color: #ea5a3f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_87375_row5_col0 {\n",
       "  background-color: #fca26d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_87375_row5_col1 {\n",
       "  background-color: #b70503;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_87375\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_87375_level0_col0\" class=\"col_heading level0 col0\" >Mean</th>\n",
       "      <th id=\"T_87375_level0_col1\" class=\"col_heading level0 col1\" >STD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_87375_level0_row0\" class=\"row_heading level0 row0\" >DecisionTree</th>\n",
       "      <td id=\"T_87375_row0_col0\" class=\"data row0 col0\" >0.62</td>\n",
       "      <td id=\"T_87375_row0_col1\" class=\"data row0 col1\" >0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_87375_level0_row1\" class=\"row_heading level0 row1\" >GradientBoost</th>\n",
       "      <td id=\"T_87375_row1_col0\" class=\"data row1 col0\" >0.66</td>\n",
       "      <td id=\"T_87375_row1_col1\" class=\"data row1 col1\" >0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_87375_level0_row2\" class=\"row_heading level0 row2\" >LGBM</th>\n",
       "      <td id=\"T_87375_row2_col0\" class=\"data row2 col0\" >0.66</td>\n",
       "      <td id=\"T_87375_row2_col1\" class=\"data row2 col1\" >0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_87375_level0_row3\" class=\"row_heading level0 row3\" >LogisticRegression</th>\n",
       "      <td id=\"T_87375_row3_col0\" class=\"data row3 col0\" >0.64</td>\n",
       "      <td id=\"T_87375_row3_col1\" class=\"data row3 col1\" >0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_87375_level0_row4\" class=\"row_heading level0 row4\" >RandomForest</th>\n",
       "      <td id=\"T_87375_row4_col0\" class=\"data row4 col0\" >0.66</td>\n",
       "      <td id=\"T_87375_row4_col1\" class=\"data row4 col1\" >0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_87375_level0_row5\" class=\"row_heading level0 row5\" >SVM</th>\n",
       "      <td id=\"T_87375_row5_col0\" class=\"data row5 col0\" >0.64</td>\n",
       "      <td id=\"T_87375_row5_col1\" class=\"data row5 col1\" >0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff408d40d0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accuracy = pd.DataFrame(columns=['model'])\n",
    "\n",
    "for model in models:\n",
    "    di = results[model]['accuracy']\n",
    "    di['model'] = model\n",
    "    df_accuracy = pd.concat([df_accuracy, pd.DataFrame(di, index=[0])])\n",
    "\n",
    "# TODO apriori weighting of the classes\n",
    "df_accuracy.melt('model').groupby(['model']).mean(numeric_only=True).rename(columns={'value': 'Mean'}).join(df_accuracy.melt('model').groupby(['model']).std(numeric_only=True).rename(columns={'value': 'STD'})).style.format(\"{:.2f}\").background_gradient(cmap='OrRd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table III from MIC:\n",
    "\n",
    "<img src=\"../imgs/MIC-table-iii.png\" alt=\"MIC Table III\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threats of validity\n",
    "\n",
    "We have some known threats of validity that we will to address in bullet points:\n",
    "\n",
    "* As of now, the final reported accuracy is not weighted against how many samples there are in each category. Doing so might improve or worsen the accuracy.\n",
    "* The models chosen by MIC have been tweaked to work well on the IRMAS dataset. We have not tweaked our parameters to work well on the OpenMIC dataset.\n",
    "* OpenMIC contains 20 000 audio samples, but annotations are missing. This means that the dataset is not complete. We leaned heavily on the modelling baseline from the OPENMIC's Official Github, and it filters many samples with missing annotations. Reporting the support of each instrument gives a better picture of the dataset. However, one should keep in mind that, while IRMAS might have fewer samples, it is complete and a better fit for the approach chosen by MIC.\n",
    "* Flute is not working for the SVM model. This could be due to a bug in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Future Work\n",
    "\n",
    "We have replicated the results from MIC, and we have evaluated our models. The most time-consuming part was understanding the differences between OpenMIC-2018 and IRMAS, which to some extent took its toll on the quality of our analysis. The absolute goal of this project is to pass DT2470 Music Informatics, and we believe we have achieved what we set out to do in the beginning of the project. Going forward, it would be interesting to see if we can improve the results. We have a few ideas:\n",
    "\n",
    "* Tweak model parameters.\n",
    "* Besides only using the mean of all features, we have also saved the standard deviation. Including them might make more expressive models.\n",
    "* Set up a weighted accuracy, where the accuracy is weighted against the number of samples in each category.\n",
    "* See what happens when we include all 20 instruments in the OpenMIC dataset.\n",
    "* Set up a pipeline to test with audio outside of the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
